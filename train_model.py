# -*- coding: utf-8 -*-
"""train_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18k2vr9i7JzHqacziYTzo9eza1qVVeDRr
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import accuracy_score, classification_report
import joblib

# Load datasets
tsunami_data = pd.read_csv("cleaned_tsunami_dataset.csv")
flood_data = pd.read_csv("cleaned_flood_dataset.csv")
earthquake_data = pd.read_csv("cleaned_earthquake_1995-2023.csv")

# Standardize column names
tsunami_data.rename(columns={'LATITUDE': 'latitude', 'LONGITUDE': 'longitude', 'EQ_MAGNITUDE': 'magnitude'}, inplace=True)
earthquake_data.rename(columns={'magnitude': 'magnitude', 'latitude': 'latitude', 'longitude': 'longitude', 'depth': 'depth'}, inplace=True)

# Fix missing depth value for tsunami (use median from tsunami dataset instead of earthquake dataset)
if 'depth' in tsunami_data.columns:
    tsunami_data['depth'].fillna(tsunami_data['depth'].median(), inplace=True)
else:
    tsunami_data['depth'] = tsunami_data['magnitude']  # Approximate depth if missing

# Select relevant features
flood_data = flood_data[['floodprobability', 'monsoonintensity', 'urbanization', 'climatechange']]
flood_data.rename(columns={'floodprobability': 'magnitude', 'monsoonintensity': 'depth', 'urbanization': 'latitude', 'climatechange': 'longitude'}, inplace=True)

tsunami_data = tsunami_data[['magnitude', 'depth', 'latitude', 'longitude']]
earthquake_data = earthquake_data[['magnitude', 'depth', 'latitude', 'longitude']]

# Add target labels
tsunami_data['target'] = 'tsunami'
flood_data['target'] = 'flood'
earthquake_data['target'] = 'earthquake'

# Combine datasets
combined_data = pd.concat([tsunami_data, flood_data, earthquake_data], ignore_index=True)

# Balance the dataset (undersampling to equalize disaster type distribution)
us = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = us.fit_resample(combined_data[['magnitude', 'depth', 'latitude', 'longitude']], combined_data['target'])

# Scale features
scaler = StandardScaler()
X_resampled_scaled = scaler.fit_transform(X_resampled)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, random_state=42)

# Train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Save the trained model with scaler
joblib.dump((model, scaler), "disaster_model.pkl")

# Evaluate model
predictions = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, predictions))
print("Classification Report:\n", classification_report(y_test, predictions))
print("Model trained successfully and saved as 'disaster_model.pkl'")